# SPDX-FileCopyrightText: Copyright (c) 2021 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause

"""
è’¸é¦æ¨¡å‹æµ‹è¯•è„šæœ¬
ä¸“é—¨ç”¨äºæµ‹è¯•å¤šæ•™å¸ˆè’¸é¦è®­ç»ƒåçš„å­¦ç”Ÿæ¨¡å‹
"""

from legged_gym import LEGGED_GYM_ROOT_DIR
import os
import sys

from legged_gym.envs import *
from legged_gym.utils import get_args, task_registry
from terrain_base.config import terrain_config

import torch
import faulthandler

def play_distill(args):
    """
    è’¸é¦æ¨¡å‹æµ‹è¯•å‡½æ•°
    åŠ è½½è’¸é¦è®­ç»ƒåçš„å­¦ç”Ÿæ¨¡å‹å¹¶åœ¨ç¯å¢ƒä¸­è¿è¡Œæœºå™¨äºº
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
    """
    faulthandler.enable()
    
    # è·å–ç¯å¢ƒé…ç½®å’Œè®­ç»ƒé…ç½®
    env_cfg, train_cfg = task_registry.get_cfgs(name=args.task)
    
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒå‚æ•°
    env_cfg.env.num_envs = 10  # å¹¶è¡Œç¯å¢ƒæ•°é‡
    env_cfg.env.episode_length_s = 1000  # æ¯ä¸ªå›åˆçš„æœ€å¤§æ—¶é•¿ï¼ˆç§’ï¼‰
    env_cfg.commands.resampling_time = 60  # å‘½ä»¤é‡é‡‡æ ·æ—¶é—´é—´éš”
    env_cfg.rewards.is_play = True  # æ ‡è®°ä¸ºæ¸¸æˆ/æµ‹è¯•æ¨¡å¼
    
    # è®¾ç½®åœ°å½¢å‚æ•° - æµ‹è¯•å¤šç§åœ°å½¢
    env_cfg.terrain.num_rows = 5  # åœ°å½¢ç½‘æ ¼è¡Œæ•°
    env_cfg.terrain.num_cols = 10  # åœ°å½¢ç½‘æ ¼åˆ—æ•°
    env_cfg.terrain.max_init_terrain_level = 2  # æœ€å¤§åˆå§‹åœ°å½¢éš¾åº¦ç­‰çº§
    
    # è®¾ç½®å™ªå£°å’ŒåŸŸéšæœºåŒ–å‚æ•°
    env_cfg.noise.add_noise = True  # æ·»åŠ å™ªå£°
    env_cfg.domain_rand.randomize_friction = True  # éšæœºåŒ–æ‘©æ“¦ç³»æ•°
    env_cfg.domain_rand.push_robots = False  # ä¸æ¨æœºå™¨äºº
    
    print("ğŸš€ åˆ›å»ºæµ‹è¯•ç¯å¢ƒ...")
    # å‡†å¤‡ç¯å¢ƒ
    env, _ = task_registry.make_env(name=args.task, args=args, env_cfg=env_cfg)
    obs = env.get_observations()  # è·å–åˆå§‹è§‚æµ‹ï¼ˆå­—å…¸æ ¼å¼ï¼‰
    
    print("ğŸ“¦ åŠ è½½è’¸é¦æ¨¡å‹...")
    # åŠ è½½è’¸é¦æ¨¡å‹
    train_cfg.runner.resume = True
    distill_runner, train_cfg, log_pth = task_registry.make_alg_runner(
        env=env, 
        name=args.task, 
        args=args, 
        train_cfg=train_cfg, 
        return_log_dir=True
    )
    
    # è·å–å­¦ç”Ÿç­–ç•¥ï¼ˆç”¨äºæ¨ç†ï¼‰
    student_policy = distill_runner.get_inference_policy(device=env.device)
    
    print("ğŸ® å¼€å§‹æµ‹è¯•...")
    print(f"ğŸ“Š æµ‹è¯•ç¯å¢ƒæ•°é‡: {env.num_envs}")
    print(f"ğŸ—ºï¸  åœ°å½¢ç±»å‹: {torch.unique(env.env_class)}")
    
    # åˆå§‹åŒ–åŠ¨ä½œå¼ é‡
    actions = torch.zeros(env.num_envs, 12, device=env.device, requires_grad=False)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_steps = 0
    episode_rewards = torch.zeros(env.num_envs, device=env.device)
    
    # ä¸»å¾ªç¯ï¼šè¿è¡Œæœºå™¨äºº
    for i in range(10 * int(env.max_episode_length)):
        # æ£€æŸ¥è§‚æµ‹æ ¼å¼
        if isinstance(obs, dict):
            # ä½¿ç”¨å­¦ç”Ÿè§‚æµ‹ï¼ˆpolicyç»„ï¼‰
            student_obs = obs["policy"]
        else:
            # å¦‚æœæ˜¯å¼ é‡æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
            student_obs = obs
        
        # ä½¿ç”¨å­¦ç”Ÿç­–ç•¥ç”ŸæˆåŠ¨ä½œ
        with torch.no_grad():
            actions = student_policy(student_obs.detach())
        
        # æ‰§è¡ŒåŠ¨ä½œï¼Œè·å–æ–°çš„è§‚æµ‹å’Œå¥–åŠ±
        obs, _, rewards, dones, infos = env.step(actions.detach())
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_steps += 1
        episode_rewards += rewards
        
        # æ¯1000æ­¥æ‰“å°ä¸€æ¬¡ç»Ÿè®¡ä¿¡æ¯
        if total_steps % 1000 == 0:
            avg_reward = episode_rewards.mean().item() / 1000
            print(f"æ­¥æ•°: {total_steps:6d} | å¹³å‡å¥–åŠ±: {avg_reward:6.3f} | åœ°å½¢ç±»å‹: {torch.unique(env.env_class).cpu().numpy()}")
            episode_rewards.zero_()
    
    print("âœ… æµ‹è¯•å®Œæˆï¼")

if __name__ == '__main__':
    args = get_args()
    play_distill(args) 